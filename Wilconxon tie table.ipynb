{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations, product\n",
    "from sanitize_ml_labels import sanitize_ml_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = \"./wilcoxon/\"\n",
    "os.makedirs(RESULT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared = lambda x: (y for y in product(list(x), list(x)) if y[0] != y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = lambda x: x.Accuracy.values\n",
    "auprc = lambda x: x.AUPRC.values\n",
    "auroc = lambda x: x.AUROC.values\n",
    "metrics = {\n",
    "    sanitize_ml_labels(\"accuracy\"):accuracy,\n",
    "    sanitize_ml_labels(\"auprc\"):auprc,\n",
    "    sanitize_ml_labels(\"auroc\"):auroc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilcoxon_test(x, y, p_threshold=0.05):\n",
    "    diff = x - y\n",
    "    if np.isclose(diff, 0).all():\n",
    "        return {\n",
    "        \"win\":0,\n",
    "        \"tie\":1,\n",
    "        \"losses\":0,\n",
    "        \"pvalue\":0,\n",
    "        }\n",
    "    \n",
    "    stats, pvalue = wilcoxon(x, y)\n",
    "    if pvalue <= p_threshold:\n",
    "        if (diff > 0).mean() > 0.5:\n",
    "            win = 1\n",
    "            lose = 0\n",
    "        else:\n",
    "            win = 0\n",
    "            lose = 1\n",
    "        tie = 0\n",
    "    else:\n",
    "        tie = 1\n",
    "        win = 0\n",
    "        lose = 0\n",
    "    return {\n",
    "        \"win\":win,\n",
    "        \"tie\":tie,\n",
    "        \"losses\":lose,\n",
    "        \"pvalue\":pvalue,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_2_models(d1, d2, extract_metric):\n",
    "    x = extract_metric(d1)\n",
    "    y = extract_metric(d2)\n",
    "    \n",
    "    assert wilcoxon_test(x, y)[\"win\"] == wilcoxon_test(y, x)[\"losses\"]\n",
    "    assert wilcoxon_test(x, y)[\"tie\"] == wilcoxon_test(y, x)[\"tie\"]\n",
    "    \n",
    "    return wilcoxon_test(x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_df(df):\n",
    "    df.columns = sanitize_ml_labels(df.columns)\n",
    "    df.index.names = list(\n",
    "        map(lambda x: x[:-1] if x[-1].isdigit() else x , \n",
    "            sanitize_ml_labels(df.index.names)\n",
    "        )\n",
    "    )\n",
    "    for col in df.columns[df.dtypes == object]:\n",
    "        df[col] = sanitize_ml_labels(df[col])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex(df, name, task, metric=None):\n",
    "    path = RESULT_PATH+f\"{name}_{task}\"\n",
    "    caption = f\"Win-Tie-Losses table for {sanitize_ml_labels(task)} obtained from Wilcoxon signed-rank test.\"\n",
    "    label = f\"tab:{task}_{name}\"\n",
    "    if metric:\n",
    "        path += \"_\" + metric\n",
    "        caption = caption[:-1] + f\"on {metric}.\"\n",
    "        \n",
    "    df = sanitize_df(df)\n",
    "    df.to_csv(f\"{path}.csv\")\n",
    "    #display(df)\n",
    "    df = df.reset_index()\n",
    "    df.columns = [\n",
    "            \"\\\\textbf{%s}\"%x\n",
    "            for x in df.columns\n",
    "        ]\n",
    "    result = df.to_latex(\n",
    "        index=False,\n",
    "        column_format=\"|{}|\".format(\n",
    "                \"|\".join(\"c\" * len(df.columns))\n",
    "            ),\n",
    "        escape=False,\n",
    "        )\n",
    "    result = result.replace(r\"\\end{tabular}\", r\"\\end{tabular}\"+f\"\\n\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\")\n",
    "    result = result.replace(\"\\\\toprule\", \"\") \n",
    "    result = result.replace(\"\\\\midrule\", \"\")\n",
    "    result = result.replace(\"\\\\bottomrule\", \"\")\n",
    "    result = result.replace(\"\\\\\\\\\\n\", \"\\\\\\\\\\n\\\\hline\\n\")\n",
    "    with open(f\"{path}.tex\", \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[H]\\n\\\\centering\\n\"+result+r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabella win tie loss per modello per dati di training (12, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tables(df, task, name=\"model_training_data_comparison\"):\n",
    "    combined_group = {\n",
    "        key:val\n",
    "        for key, val in df.groupby([\"Model\", \"Trained on\"])\n",
    "    }\n",
    "    res = pd.DataFrame([\n",
    "        {\n",
    "            \"metric\":metric,\n",
    "            \"model1\":m1,\n",
    "            \"train1\":t1,\n",
    "            \"model2\":m2,\n",
    "            \"train2\":t2,\n",
    "            **compare_2_models(combined_group[(m1, t1)], combined_group[(m2, t2)], metric_function),\n",
    "        }\n",
    "        for metric, metric_function in metrics.items()\n",
    "        for (m1, t1), (m2, t2) in squared(list(product(set(df.Model), set(df[\"Trained on\"]))))\n",
    "    ])\n",
    "    for metric in metrics.keys():\n",
    "        r = res[res.metric == metric].drop(columns=\"pvalue\").groupby([\"train1\", \"model1\"]).sum()\n",
    "        df_to_latex(r, name, task, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risultato wilcoxon per tipo di dati di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tables(df, task, name=\"training_data_comparison\"):\n",
    "    train_groupby = {\n",
    "        key:val\n",
    "        for key, val in df.groupby([\"Trained on\"])\n",
    "    }\n",
    "    res = pd.DataFrame([\n",
    "        {\n",
    "            \"metric\":metric,\n",
    "            \"train1\":t1,\n",
    "            \"train2\":t2,\n",
    "            **compare_2_models(train_groupby[t1], train_groupby[t2], metric_function),\n",
    "        }\n",
    "        for metric, metric_function in metrics.items()\n",
    "        for t1, t2 in combinations(set(df[\"Trained on\"]), 2)\n",
    "\n",
    "    ])\n",
    "    res = res.set_index(\"metric\")\n",
    "    df_to_latex(res, name, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabella win tie losses per modello (6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_total_tables(df, task, name=\"model_comparison\", models=None):\n",
    "    if not models:\n",
    "        models = df.Model.unique()\n",
    "    model_groupby = {\n",
    "        key:val\n",
    "        for key, val in df.groupby([\"Model\"])\n",
    "    }\n",
    "    res = pd.DataFrame([\n",
    "        {\n",
    "            \"metric\":metric,\n",
    "            \"model1\":m1,\n",
    "            \"model2\":m2,\n",
    "            **compare_2_models(model_groupby[m1], model_groupby[m2], metric_function),\n",
    "        }\n",
    "        for metric, metric_function in metrics.items()\n",
    "        for m1, m2 in squared(models)\n",
    "\n",
    "    ])\n",
    "    for metric in metrics.keys():\n",
    "        r = res[res.metric == metric].drop(columns=\"pvalue\").groupby([\"model1\"]).sum()\n",
    "        df_to_latex(r, name, task, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nucleotides performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucletoide_tables_wtl(df, task, name=\"nucleotides_comparison\", model_type=None):\n",
    "    df = df[df.Target != \"All nucleotides\"]\n",
    "    targets = df.Target.unique()\n",
    "    if model_type:\n",
    "        df = df[df.Model.str.contains(model_type)]\n",
    "    target_groupby = {\n",
    "        key:val\n",
    "        for key, val in df.groupby([\"Target\"])\n",
    "    }\n",
    "    res = pd.DataFrame([\n",
    "        {\n",
    "            \"metric\":metric,\n",
    "            \"target1\":t1,\n",
    "            \"target2\":t2,\n",
    "            **compare_2_models(target_groupby[t1], target_groupby[t2], metric_function),\n",
    "        }\n",
    "        for metric, metric_function in metrics.items()\n",
    "        for t1, t2 in squared(targets)\n",
    "\n",
    "    ])\n",
    "    for metric in metrics.keys():\n",
    "        r = res[res.metric == metric].drop(columns=\"pvalue\").groupby([\"target1\"]).sum()\n",
    "        df_to_latex(r, name, task, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucletoide_means(df, task, name=\"nucleotides_means\", model_type=None):\n",
    "    df = df[df.Target != \"All nucleotides\"]\n",
    "    targets = df.Target.unique()\n",
    "    if model_type:\n",
    "        df = df[df.Model.str.contains(model_type)]\n",
    "    target_groupby = {\n",
    "        key:val\n",
    "        for key, val in df.groupby([\"Target\"])\n",
    "    }\n",
    "    res = pd.DataFrame([\n",
    "        {\n",
    "            \"metric\":metric,\n",
    "            \"target\":t,\n",
    "            \"mean\":target_groupby[t][metric].mean(),\n",
    "        }\n",
    "        for metric, metric_function in metrics.items()\n",
    "        for t in targets\n",
    "\n",
    "    ])\n",
    "    for metric in metrics.keys():\n",
    "        r = res[res.metric == metric]\n",
    "        df_to_latex(r, name, task, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare best cnn and cae models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confront_best_models(df, task, name=\"best_models_comparison\", models=None):\n",
    "    if not models:\n",
    "        models = df.Model.unique()\n",
    "    model_groupby = {\n",
    "        key:val\n",
    "        for key, val in df.groupby([\"Model\"])\n",
    "    }\n",
    "    res = pd.DataFrame([\n",
    "        {\n",
    "            \"metric\":metric,\n",
    "            \"model\":\"%s vs %s\"%(m, m2),\n",
    "            **compare_2_models(model_groupby[m], model_groupby[m2], metric_function),\n",
    "        }\n",
    "        for metric, metric_function in metrics.items()\n",
    "        for m, m2 in combinations(models, 2)\n",
    "\n",
    "    ])\n",
    "    res = res.set_index(\"metric\")\n",
    "    df_to_latex(res, name, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gap_filling_data(path=\"./reports/\"):\n",
    "    df = pd.concat([\n",
    "        pd.read_csv(path+file, index_col=0)\n",
    "        for file in os.listdir(path)\n",
    "    ])\n",
    "    df = df[df.task == \"gap_filling\"]\n",
    "    df = df.drop(\"dataset\", axis=1)\n",
    "    df = df[df.run_type != \"biological validation\"]\n",
    "    df = sanitize_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = get_gap_filling_data()\n",
    "model_tables(df, \"gap_filling\")\n",
    "train_tables(df, \"gap_filling\")\n",
    "model_total_tables(df, \"gap_filling\")\n",
    "model_total_tables(df, \"gap_filling_cnn\", models=[\"CNN 200\", \"CNN 500\", \"CNN 1000\"])\n",
    "model_total_tables(df, \"gap_filling_cae\", models=[\"CAE 200\", \"CAE 500\", \"CAE 1000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confront_best_models(df, \"gap_filling\", models=[\"CNN 1000\", \"CAE 1000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zommiommy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "nucletoide_tables_wtl(df, \"gap_filling_cae\", model_type=\"CAE\")\n",
    "nucletoide_tables_wtl(df, \"gap_filling_cnn\", model_type=\"CNN\")\n",
    "nucletoide_means(df, \"gap_filling_cae\", model_type=\"CAE\")\n",
    "nucletoide_means(df, \"gap_filling_cnn\", model_type=\"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_data(path=\"./reports/\"):\n",
    "    df = pd.concat([\n",
    "        pd.read_csv(path+file, index_col=0)\n",
    "        for file in os.listdir(path)\n",
    "    ])\n",
    "    df = df[df.task == \"reconstruction\"]\n",
    "    df = df.drop(\"dataset\", axis=1)\n",
    "    df = df[df.run_type != \"biological validation\"]\n",
    "    df = sanitize_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_reconstruction_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_tables(df, \"reconstruction\")\n",
    "train_tables(df, \"reconstruction\")\n",
    "model_total_tables(df, \"reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zommiommy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "nucletoide_tables_wtl(df, \"reconstruction\")\n",
    "nucletoide_means(df, \"reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8992991097005207"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Target == \"Cytosine\"].Accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
